{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb8sVQZ51j7uzXWhxXhj+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARYANJATHAR/InfosysSpringboardProject/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djSrxCpscY68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark"
      ],
      "metadata": {
        "id": "ETqucKpI3amd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQdhnX3K3VWx"
      },
      "outputs": [],
      "source": [
        "#1.Creating a PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MyPySparkApp\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Creating a DataFrame in PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20), (\"Kunal\", 34), (\"Rajesh\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZCEv24o3o1g",
        "outputId": "ce6e1573-551a-4307-cdd7-6c3c820f0013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "| Aryan| 20|\n",
            "| Kunal| 34|\n",
            "|Rajesh| 28|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Selecting specific columns\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SelectColumns\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20), (\"Kunal\", 34), (\"Rajesh\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "name_df = df.select(\"Name\")\n",
        "name_df.show()\n",
        "\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qny93OOe4G8d",
        "outputId": "3800d8c3-c9a1-45a2-943e-c0ad89b6dc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|  Name|\n",
            "+------+\n",
            "| Aryan|\n",
            "| Kunal|\n",
            "|Rajesh|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Filtering Data\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FilterData\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20), (\"Kunal\", 34), (\"Rajesh\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "filtered_df = df.filter(df[\"Age\"] > 27)\n",
        "filtered_df.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "OmFIYwAD4UWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4abf9d-f700-4919-e60c-6d867d319b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "| Kunal| 34|\n",
            "|Rajesh| 28|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Basic Aggregate Function\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Aggregations\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20), (\"Kunal\", 34), (\"Rajesh\", 28),(\"Karan\", 30), (\"Sumit\", 12)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "average_age = df.agg(F.avg(\"Age\"))\n",
        "average_age.show()\n",
        "\n",
        "max_age = df.agg(F.max(\"Age\"))\n",
        "max_age.show()\n",
        "\n",
        "count_all = df.agg(F.count(\"*\"))\n",
        "count_all.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJBsnLAM5dPc",
        "outputId": "490c7359-c4c2-4640-ce7d-f1bf7c805c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|avg(Age)|\n",
            "+--------+\n",
            "|    24.8|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|max(Age)|\n",
            "+--------+\n",
            "|      34|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       5|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Grouping Data\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"GroupBy\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20, \"Pune\"), (\"Kunal\", 34, \"Mumbai\"), (\"Rajesh\", 28, \"Pune\"),(\"Karan\", 30,\"Mumbai\"), (\"Sumit\", 12,\"Kolkata\")]\n",
        "\n",
        "columns = [\"Name\", \"Age\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "city_count = df.groupBy(\"City\").agg(F.count(\"*\").alias(\"Total people\")).sort(\"City\")\n",
        "city_count.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9LajwII5vHJ",
        "outputId": "bf91b10b-4c51-460f-fd56-c5d186d114da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|   City|Total people|\n",
            "+-------+------------+\n",
            "|Kolkata|           1|\n",
            "| Mumbai|           2|\n",
            "|   Pune|           2|\n",
            "+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Reading Data\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
        "\n",
        "\n",
        "data = [(\"Aryan\", 20, \"Pune\"), (\"Kunal\", 34, \"Mumbai\"), (\"Rajesh\", 28, \"Pune\"),(\"Karan\", 30,\"Mumbai\"), (\"Sumit\", 12,\"Kolkata\")]\n",
        "\n",
        "with open(\"data.csv\", \"w\") as f:\n",
        "  f.write(\"Name,Age,City\\n\")\n",
        "  for row in data:\n",
        "      f.write(f\"{row[0]},{row[1]},{row[2]}\\n\")\n",
        "\n",
        "\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "df.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpe2Fs3i7_dk",
        "outputId": "5f765802-f640-4203-c4b6-a04255428f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+-------+\n",
            "|  Name|Age|   City|\n",
            "+------+---+-------+\n",
            "| Aryan| 20|   Pune|\n",
            "| Kunal| 34| Mumbai|\n",
            "|Rajesh| 28|   Pune|\n",
            "| Karan| 30| Mumbai|\n",
            "| Sumit| 12|Kolkata|\n",
            "+------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Adding a Column\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AddColumn\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20, ), (\"Kunal\", 34, ), (\"Rajesh\", 28, ),(\"Karan\", 30,), (\"Sumit\", 12,)]\n",
        "\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df_with_age_plus_10 = df.withColumn(\"Age_plus_10\", df[\"Age\"] + 10)\n",
        "df_with_age_plus_10.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzcp6N0T8PhO",
        "outputId": "8dcc08c9-4788-4d8b-ad82-f13a2990a3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+-----------+\n",
            "|  Name|Age|Age_plus_10|\n",
            "+------+---+-----------+\n",
            "| Aryan| 20|         30|\n",
            "| Kunal| 34|         44|\n",
            "|Rajesh| 28|         38|\n",
            "| Karan| 30|         40|\n",
            "| Sumit| 12|         22|\n",
            "+------+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Sorting Data\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SortData\").getOrCreate()\n",
        "\n",
        "data = [(\"Aryan\", 20, ), (\"Kunal\", 34, ), (\"Rajesh\", 28, ),(\"Karan\", 30,), (\"Sumit\", 12,)]\n",
        "\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df_sorted_age = df.orderBy(\"Age\")\n",
        "df_sorted_age.show()\n",
        "\n",
        "df_sorted_age_desc = df.orderBy(\"Age\", ascending = False)\n",
        "df_sorted_age_desc.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9fz52JK9Ime",
        "outputId": "2a3ea463-4a72-4d99-92c3-b44be34f5d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "| Sumit| 12|\n",
            "| Aryan| 20|\n",
            "|Rajesh| 28|\n",
            "| Karan| 30|\n",
            "| Kunal| 34|\n",
            "+------+---+\n",
            "\n",
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "| Kunal| 34|\n",
            "| Karan| 30|\n",
            "|Rajesh| 28|\n",
            "| Aryan| 20|\n",
            "| Sumit| 12|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Renaming Columns\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RenameColumn\").getOrCreate()\n",
        "[(\"Aryan\", 20, ), (\"Kunal\", 34, ), (\"Rajesh\", 28, ),(\"Karan\", 30,), (\"Sumit\", 12,)]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df_renamed = df.withColumnRenamed(\"Age\", \"Person_Age\")\n",
        "df_renamed.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJeV1rwk9jTz",
        "outputId": "b989debe-310b-4416-9cff-2b69063326e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|  Name|Person_Age|\n",
            "+------+----------+\n",
            "| Aryan|        20|\n",
            "| Kunal|        34|\n",
            "|Rajesh|        28|\n",
            "| Karan|        30|\n",
            "| Sumit|        12|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdXttS_h-B13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
