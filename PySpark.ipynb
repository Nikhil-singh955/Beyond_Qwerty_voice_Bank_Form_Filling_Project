{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmNFIlyaYdb6SpuodAh+S+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-singh955/AI-Project/blob/main/PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is PySpark\n",
        "\n",
        "Ans1. PySpark is the Python API for Apache Spark. It enables Python developers to harness the power of Spark's distributed computing capabilities for big data processing and analytics while using Python, one of the most popular programming languages. PySpark provides easy access to Spark's core functionalities, including data processing, machine learning, and real-time analytics.\n"
      ],
      "metadata": {
        "id": "K4MrdNJyCZJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What are the industrial benefits of PySpark?\n",
        "\n",
        "These days, almost every industry makes use of big data to evaluate where they stand and grow. When you hear the term big data, Apache Spark comes to mind. Following are the industry benefits of using PySpark that supports Spark:\n",
        "\n",
        "# Media streaming:\n",
        "Spark can be used to achieve real-time streaming to provide personalized recommendations to subscribers. Netflix is one such example that uses Apache Spark. It processes around 450 billion events every day to flow to its server-side apps.\n",
        "\n",
        "# Finance:\n",
        "Banks use Spark for accessing and analyzing the social media profiles and in turn get insights on what strategies would help them to make the right decisions regarding customer segmentation, credit risk assessments, early fraud detection etc.\n",
        "\n",
        "# Healthcare:\n",
        "Providers use Spark for analyzing the past records of the patients to identify what health issues the patients might face posting their discharge. Spark is also used to perform genome sequencing for reducing the time required for processing genome data.\n",
        "\n",
        "# Travel Industry:\n",
        "Companies like TripAdvisor uses Spark to help users plan the perfect trip and provide personalized recommendations to the travel enthusiasts by comparing data and review from hundreds of websites regarding the place, hotels, etc.\n",
        "\n",
        "# Retail and e-commerce:\n",
        "This is one important industry domain that requires big data analysis for targeted advertising. Companies like Alibaba run Spark jobs for analyzing petabytes of data for enhancing customer experience, providing targetted offers, sales and optimizing the overall performance."
      ],
      "metadata": {
        "id": "Kpq_DYkDCo1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. How can you handle missing values in a PySpark DataFrame? Provide an example to demonstrate your approach.\n",
        "\n",
        "In PySpark, missing values can be handled using the following methods provided by the DataFrame API:\n",
        "\n",
        "fillna(): Replace missing values with a specified value.\n",
        "\n",
        "dropna(): Remove rows with missing values.\n",
        "\n",
        "replace(): Replace specific values with others.\n",
        "\n",
        "Custom Imputation: Use a calculated value (like mean or median) to fill missing data"
      ],
      "metadata": {
        "id": "5ERqyDIAGQ24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"MissingValuesExample\").getOrCreate()\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = [\n",
        "    (1, \"Alice\", 50),\n",
        "    (2, \"Bob\", None),\n",
        "    (3, None, 70),\n",
        "    (4, \"David\", 80),\n",
        "    (5, None, None)\n",
        "]\n",
        "columns = [\"ID\", \"Name\", \"Score\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "F8ioZBRHGb0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is the difference between RDD, DataFrame, and Dataset?\n",
        "\n",
        "RDD: Low-level, unstructured, and requires manual serialization/deserialization.\n",
        "\n",
        "DataFrame: High-level, structured, optimized API with support for SQL queries.\n",
        "\n",
        "Dataset: Combines the best of RDDs and DataFrames with strong typing (available only in Scala and Java)."
      ],
      "metadata": {
        "id": "LU8SOsjtG_jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. How can you perform filtering in a PySpark DataFrame?"
      ],
      "metadata": {
        "id": "Umxw1ajBHQ--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"Age\"] > 25).show()\n"
      ],
      "metadata": {
        "id": "UtQ-UPWqHU8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What are transformations and actions in PySpark?\n",
        "\n",
        "Transformations: Lazy operations that define a computation plan (e.g., map(), filter()).\n",
        "\n",
        "Actions: Trigger the execution and return results (e.g., collect(), count())."
      ],
      "metadata": {
        "id": "Q7tCh2UfHnqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. How do you join two DataFrames in PySpark?\n"
      ],
      "metadata": {
        "id": "TUrCm6JxHzOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, df1[\"ID\"] == df2[\"ID\"], \"inner\").show()\n"
      ],
      "metadata": {
        "id": "DKsWFgqoH2ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. What is SparkSession?\n",
        "\n",
        "SparkSession is the entry point for PySpark applications. It provides methods to create DataFrames, access Spark functionality, and manage the Spark application lifecycle."
      ],
      "metadata": {
        "id": "JOQqe5fAH5Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. How do you read and write data in PySpark?\n",
        "\n"
      ],
      "metadata": {
        "id": "AozOg3QIIIfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# READ\n",
        "\n",
        "df = spark.read.csv(\"file_path.csv\", header=True)\n",
        "\n",
        "# WRITE\n",
        "\n",
        "df.write.csv(\"output_path\", header=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "8s48MU2gINpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. What are the benefits of using PySpark over Pandas?\n",
        "\n",
        "Handles large datasets that don't fit into memory.\n",
        "\n",
        "Optimized for distributed computing.\n",
        "\n",
        "Supports fault tolerance and parallel processing.\n"
      ],
      "metadata": {
        "id": "h-5kBwG-IdG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Explain PySpark's partitioning.\n",
        "\n",
        "Partitioning divides data into smaller chunks (partitions) to process them in parallel, improving performance."
      ],
      "metadata": {
        "id": "0qg4SFGBItRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. How do you handle null values in PySpark?"
      ],
      "metadata": {
        "id": "Z5j-3C7uI4KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill null values:\n",
        "\n",
        "df.fillna({\"column_name\": \"default_value\"}).show()\n",
        "\n",
        "\n",
        "# Drop rows with nulls:\n",
        "\n",
        "df.dropna().show()\n"
      ],
      "metadata": {
        "id": "8GfVS4AkI8fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 13. How can you monitor and debug a PySpark application?\n",
        "\n",
        "\n",
        "Use the Spark UI for monitoring jobs, stages, and tasks.\n",
        "\n",
        "Enable event logs to debug performance issues.\n",
        "\n",
        "Utilize explain() to view execution plans:"
      ],
      "metadata": {
        "id": "hg-Y7uSTJYSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.explain()\n"
      ],
      "metadata": {
        "id": "DbzEmoVwJdkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. What is PySpark MLlib?\n",
        "\n",
        "\n",
        "PySpark MLlib is a library for scalable machine learning in Spark, providing tools for classification, regression, clustering, and collaborative filtering.\n",
        "\n"
      ],
      "metadata": {
        "id": "mWlWh5_4JNUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15 What is PySpark UDF?\n",
        "\n",
        "UDF stands for User Defined Functions. In PySpark, UDF can be created by creating a python function and wrapping it with PySpark SQL’s udf() method and using it on the DataFrame or SQL. These are generally created when we do not have the functionalities supported in PySpark’s library and we have to use our own logic on the data. UDFs can be reused on any number of SQL expressions or DataFrames."
      ],
      "metadata": {
        "id": "oStvbZg4FmcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. What do you understand about PySpark DataFrames?\n",
        "\n",
        "PySpark DataFrame is a distributed collection of well-organized data that is equivalent to tables of the relational databases and are placed into named columns. PySpark DataFrame has better optimisation when compared to R or python. These can be created from different sources like Hive Tables, Structured Data Files, existing RDDs, external databases etc as shown in the image below:"
      ],
      "metadata": {
        "id": "JQTTDO5hGUi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Is PySpark faster than pandas?\n",
        "\n",
        "PySpark supports parallel execution of statements in a distributed environment, i.e on different cores and different machines which are not present in Pandas. This is why PySpark is faster than pandas."
      ],
      "metadata": {
        "id": "gcWgJcvMIS4F"
      }
    }
  ]
}